
import jsonimport numpy as npimport pandas as pdimport torchfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipelinefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import (    accuracy_score,    precision_recall_fscore_support,)class FinancialQAModel:    def __init__(self, model_name="deepset/roberta-base-squad2"):        """        Initialize the Hugging Face pipeline for question answering.        """        print(f"Initializing model: {model_name}")        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)        self.qa_pipeline = pipeline("question-answering", model=self.model, tokenizer=self.tokenizer)    def load_dataset(self, filepath):        """        Load the ConvFinQA dataset from a JSON file.        """        try:            with open(filepath, 'r') as f:                data = json.load(f)            print(f"Total entries in dataset: {len(data)}")            # Filter out entries without the 'qa' field            data = [entry for entry in data if 'qa' in entry]            # Extract questions, answers, and contexts            questions = [entry['qa']['question'] for entry in data]            answers = [entry['qa']['answer'] for entry in data]            contexts = [' '.join(entry.get('pre_text', []) + entry.get('post_text', [])) for entry in data]            print(f"Loaded {len(questions)} valid questions.")            return questions, answers, contexts        except Exception as e:            print(f"Error loading dataset: {e}")            return [], [], []    def evaluate_model(self, questions, answers, contexts, batch_size=16):        """        Evaluate the QA model on the dataset and compute metrics.        """        normalized_answers = [self._normalize_answer(ans) for ans in answers]        X_train, X_test, y_train, y_test = train_test_split(            list(zip(questions, contexts)),            normalized_answers,            test_size=0.2,            random_state=42        )        predictions = []        for i in range(0, len(X_test), batch_size):            batch = X_test[i:i+batch_size]            batch_predictions = []            for question, context in batch:                try:                    result = self.qa_pipeline({"question": question, "context": context})                    normalized_pred = self._normalize_answer(result.get('answer', ''))                    batch_predictions.append(normalized_pred)                except Exception as e:                    print(f"Prediction error: {e}")                    batch_predictions.append(None)            predictions.extend(batch_predictions)        # Filter out invalid predictions        valid_indices = [i for i, pred in enumerate(predictions) if pred is not None]        valid_predictions = [predictions[i] for i in valid_indices]        valid_true_answers = [y_test[i] for i in valid_indices]        # Compute metrics        metrics = self._compute_metrics(valid_true_answers, valid_predictions)        return metrics    def _compute_metrics(self, true_answers, predicted_answers):        """        Compute evaluation metrics for the QA task.        """        accuracy = accuracy_score(true_answers, predicted_answers)        precision, recall, f1, _ = precision_recall_fscore_support(            true_answers, predicted_answers, average='weighted', zero_division=0        )        return {            "accuracy": accuracy,            "precision": precision,            "recall": recall,            "f1_score": f1        }    def _normalize_answer(self, answer):        """        Normalize an answer by removing punctuation, whitespace, and articles.        """        import re        import string        answer = answer.lower()        answer = re.sub(f"[{string.punctuation}]", "", answer)        answer = re.sub(r"\s+", " ", answer).strip()        return answer    def generate_report(self, metrics):        """        Generate a performance report.        """        report = "# Financial QA Model Performance Report\n\n"        report += "## Evaluation Metrics\n\n"        for metric, value in metrics.items():            report += f"- **{metric.capitalize()}**: {value:.2%}\n"        report += "\n## Model report\n"        report += "- The model's performance is summarized above.\n"        report += "- fine-tuning is needed on domain-specific data for better results.\n"        return reportdef main():    filepath = '/content/train.json'  # Replace with your dataset path    model = FinancialQAModel()    # Load dataset    questions, answers, contexts = model.load_dataset(filepath)    # Evaluate the model    metrics = model.evaluate_model(questions, answers, contexts, batch_size=16)    # Generate and print the report    report = model.generate_report(metrics)    print(report)    # Save the report to a file    with open('financial_qa_report.md', 'w') as f:        f.write(report)if __name__ == "__main__":    main()

 Initializing model: deepset/roberta-base-squad2
Total entries in dataset: 3037
Loaded 2109 valid questions.
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.
  warnings.warn(
# Financial QA Model Performance Report

## Evaluation Metrics

- **Accuracy**: 0.24%
- **Precision**: 0.09%
- **Recall**: 0.24%
- **F1_score**: 0.14%

## Model report
- The model's performance is summarized above.
- fine-tuning is needed on domain-specific data for better results.
